{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def linear(in_array: np.ndarray, weight: np.ndarray, bias: np.ndarray) -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    linear transformation: y = xW^T + b\n",
    "\n",
    "    Args:\n",
    "        in_array: input array\n",
    "        weight: input weight\n",
    "        bias: input bias\n",
    "\n",
    "    Shape:\n",
    "        - Input: (N, in_features)\n",
    "        - Weight: (out_features, in_features)\n",
    "        - Bias: (out_features)\n",
    "        - Output: (N, out_features)\n",
    "    \"\"\"\n",
    "\n",
    "    assert weight.shape[-1] == in_array.shape[-1], \"ERR : input and weight dimensions not matched.\"\n",
    "\n",
    "    return np.matmul(in_array, weight.T) + bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self):\n",
    "        return\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def regression_initializer(size: Tuple, cate: str='normal') -> Dict:\n",
    "    \"\"\"initialize list with size `length`,default to `normal` \"\"\"\n",
    "    assert size is not None, \"size is None.\"\n",
    "\n",
    "    cate_type = cate.lower()\n",
    "    initial_list = ['normal', 'uniform']\n",
    "    assert cate_type in initial_list, NotImplemented\n",
    "\n",
    "    if cate == 'normal':\n",
    "        return {\n",
    "            'W': np.random.normal(loc=0., scale=1., size=size),\n",
    "            'b': np.random.normal(loc=0., scale=1., size=size[0])\n",
    "        }\n",
    "    elif cate == 'uniform':\n",
    "        a = 1. / size[-1]\n",
    "        return {\n",
    "            'W': np.random.uniform(low=-a, high=a, size=size),\n",
    "            'b': np.random.normal(loc=0., scale=1., size=size[0])\n",
    "        }\n",
    "    else:\n",
    "        assert NotImplemented"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred: np.ndarray, y_std: np.ndarray, n: int):\n",
    "    \"\"\"Mean Squared Error, a.k.a MSE\"\"\"\n",
    "    return np.sum((y_pred - y_std)**2) / n\n",
    "\n",
    "def loss_fn(cate: str='mse'):\n",
    "    \"\"\"loss functions\"\"\"\n",
    "    if cate=='mse':\n",
    "        return mean_squared_error\n",
    "    else:\n",
    "        return NotImplemented"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "def SGD(dw, db, lr, weight, bias):\n",
    "    \"\"\"Stochastic Gradient Descent\"\"\"\n",
    "    return {\n",
    "        'weight_updated': weight - lr * dw,\n",
    "        'bias_updated': bias - lr * db\n",
    "    }\n",
    "\n",
    "def optimizer(cate: str='SGD'):\n",
    "    \"\"\"optimizors\"\"\"\n",
    "    if cate == 'SGD':\n",
    "        return SGD\n",
    "    else:\n",
    "        return NotImplemented\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "outputs": [],
   "source": [
    "class LinearRegression(Module):\n",
    "    \"\"\"\n",
    "    linear transformations: y = xW^T + b, initializer default to `normal`\n",
    "\n",
    "    Args:\n",
    "        in_array: feature array\n",
    "\n",
    "    Shapes:\n",
    "        in_array: (N, in_features)\n",
    "        out_array: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_array: np.ndarray,\n",
    "                 loss_fn_type: str='mse',\n",
    "                 lr: float=0.00001,\n",
    "                 optimizer_type: str='SGD'):\n",
    "        # NOTICE: check in_array for batch case\n",
    "        super().__init__()\n",
    "        self.in_array = in_array\n",
    "        self.in_features = in_array.shape[-1]\n",
    "        self.out_features = 1\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer(optimizer_type)\n",
    "        self.initial = regression_initializer((self.out_features,\n",
    "                                               self.in_features),\n",
    "                                               'normal')\n",
    "        self.weight = self.initial['W']\n",
    "        self.bias = self.initial['b']\n",
    "\n",
    "        self.loss_fn = loss_fn(loss_fn_type)\n",
    "        self.loss = 99999\n",
    "\n",
    "        self.dw = 0\n",
    "        self.db = 0\n",
    "\n",
    "        self.y_pred = np.array([])\n",
    "\n",
    "    def forward(self) -> np.ndarray:\n",
    "        self.y_pred = linear(self.in_array, self.weight, self.bias)\n",
    "        return self.y_pred\n",
    "\n",
    "    def loss_reg(self, y_std):\n",
    "        self.loss = self.loss_fn(self.y_pred, y_std, self.in_features)\n",
    "\n",
    "    def backward(self, y_std):\n",
    "        diff = self.y_pred - y_std\n",
    "        self.dw = (1. / self.in_features) * np.matmul(diff.T, self.in_array)\n",
    "        self.db = (1. / self.in_features) * np.sum(diff)\n",
    "\n",
    "    def step(self):\n",
    "        param_updated = self.optimizer(self.dw, self.db, self.lr, self.weight, self.bias)\n",
    "        self.weight = param_updated['weight_updated']\n",
    "        self.bias = param_updated['bias_updated']\n",
    "\n",
    "    def print_params(self):\n",
    "        # print(f\"weight: {self.weight} \\nbias: {self.bias} \\nloss: {self.loss}\")\n",
    "        print(f\"loss: {self.loss}\")\n",
    "\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"Linear Regression Model\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [],
   "source": [
    "def training(model, epochs):\n",
    "    result = {}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.forward()\n",
    "        model.backward(labels)\n",
    "        model.loss_reg(labels)\n",
    "        model.step()\n",
    "        print(f\"Epoch: {epoch}\", end=\" \")\n",
    "        model.print_params()\n",
    "\n",
    "    result['name'] = model.__class__.__name__\n",
    "    result['weight'] = model.weight\n",
    "    result['bias'] = model.bias\n",
    "    result['loss'] = model.loss\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "def synthetic_data(w,b,num_examples):\n",
    "    \"\"\" 生成 y = Xw + b + 噪声。\"\"\"\n",
    "    X = np.random.normal(0,1,(num_examples,len(w)))\n",
    "    y = np.matmul(X,w) + b\n",
    "    y += np.random.normal(0,0.01,y.shape) # 均值为0，方差为1\n",
    "    return X, y.reshape((-1,1))\n",
    "\n",
    "true_w = np.array([i + 0.5 for i in range(23)])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w,true_b,1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 171776.41419665646\n",
      "Epoch: 1 loss: 171627.60914508803\n",
      "Epoch: 2 loss: 171478.93643035484\n",
      "Epoch: 3 loss: 171330.39593197798\n",
      "Epoch: 4 loss: 171181.98752959035\n",
      "Epoch: 5 loss: 171033.71110293674\n",
      "Epoch: 6 loss: 170885.56653187377\n",
      "Epoch: 7 loss: 170737.55369636958\n",
      "Epoch: 8 loss: 170589.67247650394\n",
      "Epoch: 9 loss: 170441.92275246794\n"
     ]
    }
   ],
   "source": [
    "linear_reg = LinearRegression(features)\n",
    "training_res = training(linear_reg, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Authentic Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "outputs": [
    {
     "data": {
      "text/plain": "         index         a         e          i          om           w  \\\n0            0  2.769165  0.076009  10.594067   80.305532   73.597694   \n1            1  2.772466  0.230337  34.836234  173.080063  310.048857   \n2            2  2.669150  0.256942  12.988919  169.852760  248.138626   \n3            3  2.361418  0.088721   7.141771  103.810804  150.728541   \n4            4  2.574249  0.191095   5.366988  141.576605  358.687607   \n...        ...       ...       ...        ...         ...         ...   \n137631  797860  3.171225  0.159119  27.098625  309.036573   19.746812   \n137632  798077  2.548410  0.076071  11.593237  246.298656  170.090810   \n137633  798189  3.146246  0.220559  17.966646  137.981403  180.898832   \n137634  799752  3.051336  0.287449  14.456779  343.917822  342.614839   \n137635  810375  2.417477  0.109001   4.525668  148.244819   31.949854   \n\n               q        ad  data_arc  n_obs_used  ...  class_APO  class_AST  \\\n0       2.558684  2.979647    8822.0        1002  ...      False      False   \n1       2.133865  3.411067   72318.0        8490  ...      False      False   \n2       1.983332  3.354967   72684.0        7104  ...      False      False   \n3       2.151909  2.570926   24288.0        9325  ...      False      False   \n4       2.082324  3.066174   63507.0        2916  ...      False      False   \n...          ...       ...       ...         ...  ...        ...        ...   \n137631  2.666623  3.675826    2373.0          50  ...      False      False   \n137632  2.354549  2.742270    3297.0          33  ...      False      False   \n137633  2.452313  3.840180    2839.0          47  ...      False      False   \n137634  2.174231  3.928440    2208.0          27  ...      False      False   \n137635  2.153970  2.680984    3458.0          25  ...      False      False   \n\n        class_ATE  class_CEN  class_IMB  class_MBA  class_MCA  class_OMB  \\\n0           False      False      False       True      False      False   \n1           False      False      False       True      False      False   \n2           False      False      False       True      False      False   \n3           False      False      False       True      False      False   \n4           False      False      False       True      False      False   \n...           ...        ...        ...        ...        ...        ...   \n137631      False      False      False       True      False      False   \n137632      False      False      False       True      False      False   \n137633      False      False      False       True      False      False   \n137634      False      False      False       True      False      False   \n137635      False      False      False       True      False      False   \n\n        class_TJN  class_TNO  \n0           False      False  \n1           False      False  \n2           False      False  \n3           False      False  \n4           False      False  \n...           ...        ...  \n137631      False      False  \n137632      False      False  \n137633      False      False  \n137634      False      False  \n137635      False      False  \n\n[137636 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>a</th>\n      <th>e</th>\n      <th>i</th>\n      <th>om</th>\n      <th>w</th>\n      <th>q</th>\n      <th>ad</th>\n      <th>data_arc</th>\n      <th>n_obs_used</th>\n      <th>...</th>\n      <th>class_APO</th>\n      <th>class_AST</th>\n      <th>class_ATE</th>\n      <th>class_CEN</th>\n      <th>class_IMB</th>\n      <th>class_MBA</th>\n      <th>class_MCA</th>\n      <th>class_OMB</th>\n      <th>class_TJN</th>\n      <th>class_TNO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2.769165</td>\n      <td>0.076009</td>\n      <td>10.594067</td>\n      <td>80.305532</td>\n      <td>73.597694</td>\n      <td>2.558684</td>\n      <td>2.979647</td>\n      <td>8822.0</td>\n      <td>1002</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2.772466</td>\n      <td>0.230337</td>\n      <td>34.836234</td>\n      <td>173.080063</td>\n      <td>310.048857</td>\n      <td>2.133865</td>\n      <td>3.411067</td>\n      <td>72318.0</td>\n      <td>8490</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2.669150</td>\n      <td>0.256942</td>\n      <td>12.988919</td>\n      <td>169.852760</td>\n      <td>248.138626</td>\n      <td>1.983332</td>\n      <td>3.354967</td>\n      <td>72684.0</td>\n      <td>7104</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2.361418</td>\n      <td>0.088721</td>\n      <td>7.141771</td>\n      <td>103.810804</td>\n      <td>150.728541</td>\n      <td>2.151909</td>\n      <td>2.570926</td>\n      <td>24288.0</td>\n      <td>9325</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2.574249</td>\n      <td>0.191095</td>\n      <td>5.366988</td>\n      <td>141.576605</td>\n      <td>358.687607</td>\n      <td>2.082324</td>\n      <td>3.066174</td>\n      <td>63507.0</td>\n      <td>2916</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>137631</th>\n      <td>797860</td>\n      <td>3.171225</td>\n      <td>0.159119</td>\n      <td>27.098625</td>\n      <td>309.036573</td>\n      <td>19.746812</td>\n      <td>2.666623</td>\n      <td>3.675826</td>\n      <td>2373.0</td>\n      <td>50</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>137632</th>\n      <td>798077</td>\n      <td>2.548410</td>\n      <td>0.076071</td>\n      <td>11.593237</td>\n      <td>246.298656</td>\n      <td>170.090810</td>\n      <td>2.354549</td>\n      <td>2.742270</td>\n      <td>3297.0</td>\n      <td>33</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>137633</th>\n      <td>798189</td>\n      <td>3.146246</td>\n      <td>0.220559</td>\n      <td>17.966646</td>\n      <td>137.981403</td>\n      <td>180.898832</td>\n      <td>2.452313</td>\n      <td>3.840180</td>\n      <td>2839.0</td>\n      <td>47</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>137634</th>\n      <td>799752</td>\n      <td>3.051336</td>\n      <td>0.287449</td>\n      <td>14.456779</td>\n      <td>343.917822</td>\n      <td>342.614839</td>\n      <td>2.174231</td>\n      <td>3.928440</td>\n      <td>2208.0</td>\n      <td>27</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>137635</th>\n      <td>810375</td>\n      <td>2.417477</td>\n      <td>0.109001</td>\n      <td>4.525668</td>\n      <td>148.244819</td>\n      <td>31.949854</td>\n      <td>2.153970</td>\n      <td>2.680984</td>\n      <td>3458.0</td>\n      <td>25</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>137636 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet = pd.read_csv('output_basic_data_preprocessing.csv')\n",
    "sheet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [],
   "source": [
    "sheet.drop('index', axis=1, inplace=True)\n",
    "label = sheet['diameter']\n",
    "dia_dropped = sheet.drop('diameter', axis=1)\n",
    "feature = dia_dropped\n",
    "\n",
    "# labels & features\n",
    "label_a = np.array(label)[:1000]\n",
    "feature_a = np.array(feature)[:1000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0 loss: 3483369172.6801386\n",
      "Epochs: 1 loss: 2.467047839942368e+21\n",
      "Epochs: 2 loss: 1.7828658641787683e+33\n",
      "Epochs: 3 loss: 1.2884268558256412e+45\n",
      "Epochs: 4 loss: 9.311097352673888e+56\n",
      "Epochs: 5 loss: 6.728867340739659e+68\n",
      "Epochs: 6 loss: 4.862762569684695e+80\n",
      "Epochs: 7 loss: 3.514181304476004e+92\n",
      "Epochs: 8 loss: 2.5395996748262812e+104\n",
      "Epochs: 9 loss: 1.8352970292576905e+116\n"
     ]
    }
   ],
   "source": [
    "linear_reg_asteroids = LinearRegression(feature_a)\n",
    "training_res = training(linear_reg_asteroids, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'LinearRegression',\n 'weight': array([[-2.472716031730591e+54, -1.302809797227986e+53,\n         -8.384598019300061e+54, -1.521811500877251e+56,\n         -1.5741729968637798e+56, -2.1102946839636763e+54,\n         -2.835137379515213e+54, -3.9397160643145045e+58,\n         -2.378918887528671e+57, -1.9153717673676083e+53,\n         -1.5220418255453887e+57, -1.610033219081791e+56,\n         -1.682660922209947e+51, -0.03311837061349168, 0.4747445679963158,\n         -0.8731717793550927, -7.305873615937039e+50,\n         -8.918370003787828e+50, -8.147698478321307e+53,\n         -3.594376423523605e+51, -5.684342578026334e+52,\n         -4.1714881574557845e+51, 0.2612445529423622]], dtype=object),\n 'bias': array([-8.82684223e+53]),\n 'loss': 1.8352970292576905e+116}"
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_res"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
